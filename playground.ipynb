{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "\n",
    "class Singleton(type):\n",
    "    _instances = {}\n",
    "    def __call__(cls, *args, **kwargs):\n",
    "        if cls not in cls._instances:\n",
    "            cls._instances[cls] = super(Singleton, cls).__call__(*args, **kwargs)\n",
    "        return cls._instances[cls]\n",
    "\n",
    "class llm_wrap(metaclass=Singleton):\n",
    "    def __init__(self, config_file=\"ollama_config.json\"):\n",
    "        self.loaded = False\n",
    "        self.config_data = self.load_config(config_file)\n",
    "        self.load_response = self.load_model(self.config_data[\"LLM\"])\n",
    "        if self.load_response.status_code == 200: self.loaded = True\n",
    "\n",
    "    def load_config(self, config_file):\n",
    "        \"\"\"\n",
    "        Loads environment-variable-like keys from a JSON file and sets them.\n",
    "        \"\"\"\n",
    "        with open(config_file, 'r') as f:\n",
    "            config_data = json.load(f)\n",
    "        for key, value in config_data.items():\n",
    "            os.environ[key] = str(value)\n",
    "        return config_data\n",
    "\n",
    "    def load_model(self, model_name: str):\n",
    "        \"\"\"\n",
    "        Instantiates an Ollama client for a given model.\n",
    "        Because environment variables are already set,\n",
    "        Ollama respects those concurrency/queue settings.\n",
    "        \"\"\"\n",
    "        url = self.config_data[\"LOAD_MODEL_API_PATH\"]\n",
    "        payload = {\n",
    "            \"model\": model_name,\n",
    "            \"stream\": False  \n",
    "        }\n",
    "\n",
    "        response = requests.post(url, json=payload)\n",
    "        print(\"Status code:\", response.status_code)\n",
    "        print(\"Response:\", response.text)\n",
    "\n",
    "        return response\n",
    "    \n",
    "    def process_prompt(self, prompt: str, chat = True):\n",
    "        \"\"\"\n",
    "        Sends a prompt to the provided Ollama client and prints the \n",
    "        streamed output in real-time.\n",
    "        \"\"\"\n",
    "        if self.loaded:\n",
    "            url = self.config_data[\"CHAT_API_PATH\"] if chat else self.config_data[\"GENERATE_API_PATH\"]\n",
    "            payload = {\n",
    "                        \"model\": self.config_data[\"LLM\"],\n",
    "                        \"messages\": [\n",
    "                            {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": prompt\n",
    "                            }\n",
    "                        ],\n",
    "                        \"stream\": True,\n",
    "                        \"options\": {\n",
    "                                    \"temperature\": self.config_data[\"TEMPERATURE\"]\n",
    "                                    }\n",
    "                        }\n",
    "            response = requests.post(url, json=payload, stream=True)\n",
    "            self.complete_message = \"\"\n",
    "            for line in response.iter_lines(decode_unicode=True):\n",
    "                if line: \n",
    "                    try:\n",
    "                        chunk = json.loads(line)\n",
    "                        print(chunk)\n",
    "                        if chunk[\"done\"]:\n",
    "                            return self.complete_message\n",
    "                        else:\n",
    "                            self.complete_message += chunk[\"message\"][\"content\"]\n",
    "                            yield self.complete_message\n",
    "                    except json.JSONDecodeError as e:\n",
    "                        print(\"Could not decode chunk:\", line)\n",
    "                        return False\n",
    "\n",
    "            # return response\n",
    "        else:\n",
    "            print(\"Failed to load the model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for response in llm_wrap().process_prompt(\"think and say something\"):\n",
    "    # print(response)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import random\n",
    "import time\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot(type=\"messages\")\n",
    "    msg = gr.Textbox()\n",
    "    clear = gr.ClearButton([msg, chatbot])\n",
    "\n",
    "    def respond(message, chat_history):\n",
    "        bot_message = random.choice([\"How are you?\", \"Today is a great day\", \"I'm very hungry\"])\n",
    "        chat_history.append({\"role\": \"user\", \"content\": message})\n",
    "        chat_history.append({\"role\": \"assistant\", \"content\": bot_message})\n",
    "        time.sleep(2)\n",
    "        return \"\", chat_history\n",
    "\n",
    "    msg.submit(respond, [msg, chatbot], [msg, chatbot])\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mhammed/Desktop/tech_projects/Verbal_Communication_Skills_Trainer/Verbal_Communication_Skills_Trainer_venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/mhammed/Desktop/tech_projects/Verbal_Communication_Skills_Trainer/Verbal_Communication_Skills_Trainer_venv/lib/python3.12/site-packages/gradio/utils.py:1024: UserWarning: Expected 3 arguments for function <function generate_tone at 0x7ba8a40c1f80>, received 1.\n",
      "  warnings.warn(\n",
      "/home/mhammed/Desktop/tech_projects/Verbal_Communication_Skills_Trainer/Verbal_Communication_Skills_Trainer_venv/lib/python3.12/site-packages/gradio/utils.py:1028: UserWarning: Expected at least 3 arguments for function <function generate_tone at 0x7ba8a40c1f80>, received 1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gradio as gr\n",
    "\n",
    "notes = [\"C\", \"C#\", \"D\", \"D#\", \"E\", \"F\", \"F#\", \"G\", \"G#\", \"A\", \"A#\", \"B\"]\n",
    "\n",
    "def generate_tone(note, octave, duration):\n",
    "    sr = 48000\n",
    "    a4_freq, tones_from_a4 = 440, 12 * (octave - 4) + (note - 9)\n",
    "    frequency = a4_freq * 2 ** (tones_from_a4 / 12)\n",
    "    duration = int(duration)\n",
    "    audio = np.linspace(0, duration, duration * sr)\n",
    "    audio = (20000 * np.sin(audio * (2 * np.pi * frequency))).astype(np.int16)\n",
    "    return sr, audio\n",
    "\n",
    "demo = gr.Interface(\n",
    "    generate_tone,\n",
    "    [\n",
    "        # gr.Dropdown(notes, type=\"index\"),\n",
    "        # gr.Slider(4, 6, step=1),\n",
    "        # gr.Textbox(value=\"1\", label=\"Duration in seconds\"),\n",
    "        gr.Audio()\n",
    "    ],\n",
    "    \"audio\",\n",
    ")\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mhammed/Desktop/tech_projects/Verbal_Communication_Skills_Trainer/Verbal_Communication_Skills_Trainer_venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from gradio import ChatMessage\n",
    "from prompt_handler import handle_prompt\n",
    "import gradio as gr\n",
    "import json\n",
    "\n",
    "class SingletonMeta(type):\n",
    "    _instances = {}\n",
    "\n",
    "    def __call__(cls, *args, **kwargs):\n",
    "        if cls not in cls._instances:\n",
    "            cls._instances[cls] = super(SingletonMeta, cls).__call__(*args, **kwargs)\n",
    "        return cls._instances[cls]\n",
    "\n",
    "class ChatInterface(metaclass=SingletonMeta):\n",
    "    def __init__(self, chat_function):\n",
    "        self.demo = gr.ChatInterface(\n",
    "            chat_function,\n",
    "            title=\"Verbal Communication Skills Trainer ðŸ’ª\",\n",
    "            type=\"messages\",\n",
    "            multimodal=True,\n",
    "            textbox=gr.MultimodalTextbox(sources=[\"microphone\", \"upload\"]),\n",
    "            # additional_inputs= gr.Audio()\n",
    "        )\n",
    "        \n",
    "\n",
    "    def launch_interface(self):\n",
    "        self.demo.launch()\n",
    "\n",
    "# Load the content from a Markdown file\n",
    "with open(\"improptu_speaking.md\", \"r\", encoding=\"utf-8\") as file:\n",
    "    md_content = file.read()\n",
    "md_content += \"\\n\"\n",
    "instructions = {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": md_content,\n",
    "    \"metadata\": None\n",
    "}\n",
    "\n",
    "def think_chat_function(message, history):\n",
    "    print(history)\n",
    "    print(type(history))\n",
    "    if history: print(type(history[0]))\n",
    "    # print(\"#######################\")\n",
    "    # print(message)\n",
    "    \n",
    "    # conv_stage = 0 if 'conv_stage' not in locals() or conv_stage>2 else conv_stage\n",
    "    # lines = instructions[\"content\"].splitlines()\n",
    "    # lines.pop()\n",
    "    # instructions[\"content\"] = \"\\n\".join(lines) + \"\\n\" + str(conv_stage)\n",
    "    history.append(instructions)\n",
    "    # message = message[\"text\"]\n",
    "    del message[\"files\"]\n",
    "    message[\"content\"] = message.pop(\"text\")\n",
    "    message[\"role\"] = \"user\"\n",
    "    history.append(message)\n",
    "    response = [ChatMessage(\n",
    "        content=\"\",\n",
    "        metadata={}\n",
    "    )]\n",
    "\n",
    "    think_stage = \"think\"\n",
    "    llm_handler = handle_prompt()\n",
    "    llm_generator = llm_handler.get_llm_output(history)\n",
    "    if llm_generator:\n",
    "        for llm_response in llm_generator:\n",
    "            # print(\"response here###############\")\n",
    "            # print(llm_response)\n",
    "            # print(\"response here###############\")\n",
    "            # llm_response_dict = json.loads(llm_response)\n",
    "            # llm_response, conv_stage = llm_response_dict[\"content\"], llm_response_dict[\"stage\"]\n",
    "            llm_response, think_stage = process_string(llm_response, think_stage)\n",
    "\n",
    "            if think_stage==\"think\":\n",
    "                response[-1].content = llm_response\n",
    "                # response[-1].metadata = {\"title\": \"Thinking\", \"id\": conv_stage, \"status\": \"pending\"}\n",
    "                response[-1].metadata = {\"title\": \"Thinking\", \"status\": \"pending\"}\n",
    "            elif think_stage==\"transition\":\n",
    "                response[-1].content = llm_response\n",
    "                response[-1].metadata[\"status\"] = \"done\"\n",
    "                response[-1].metadata[\"thought_len\"] = len(llm_response)\n",
    "                response.append(ChatMessage(content=\"\"))\n",
    "            else:\n",
    "                response[-1]=ChatMessage(content=llm_response[response[-2].metadata[\"thought_len\"]:])\n",
    "            yield response\n",
    "        # print(response)\n",
    "        # del llm_generator\n",
    "        # yield response\n",
    "        # conv_stage += 1\n",
    "        if llm_handler.embedding and llm_handler.embedding not in llm_handler.cache: llm_handler.cache[llm_handler.embedding] = response[-1].content\n",
    "    else: yield\n",
    "\n",
    "def process_string(s, think_stage):\n",
    "    if s.startswith(\"<think>\") and \"</think>\" not in s: think_stage = \"think\"\n",
    "    elif think_stage == \"think\": think_stage = \"transition\"\n",
    "    else: think_stage = \"respond\"\n",
    "    \n",
    "    s = s.replace(\"<think>\", \"\")\n",
    "    s = s.replace(\"</think>\", \"\")\n",
    "    \n",
    "    return s, think_stage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "<class 'list'>\n",
      "loading................\n",
      "{'LLM': 'deepseek-r1:14b', 'TEMPERATURE': 0.2, 'EMBEDDING_MODEL': 'nomic-embed-text:latest', 'EMBED_TEMPERATURE': 0, 'OLLAMA_MAX_LOADED_MODELS': 1, 'OLLAMA_NUM_PARALLEL': 1, 'OLLAMA_MAX_QUEUE': 0, 'LOAD_MODEL_API_PATH': 'http://localhost:11434/api/pull', 'GENERATE_API_PATH': 'http://localhost:11434/api/generate', 'CHAT_API_PATH': 'http://localhost:11434/api/chat', 'EMBEDDING_API_PATH': 'http://localhost:11434/api/embed'}\n",
      "Status code: 200\n",
      "Response: {\"status\":\"success\"}\n",
      "response###########################\n",
      "<Response [200]>\n",
      "[{'role': 'user', 'metadata': None, 'content': 'hi', 'options': None}, {'role': 'assistant', 'metadata': {'title': 'Thinking', 'status': 'done'}, 'content': 'Okay, so I\\'m trying to figure out how to respond to the user\\'s message where they said \"hi.\" The context is that I\\'m supposed to be a communication skills coach following a specific prompt template. Let me break this down step by step.\\n\\nFirst, I need to identify which stage of conversation we\\'re in. According to the prompt, there are four stages: Introduction and initial exploration, Deeper questioning or scenario-based prompts, Final evaluative questions, and Conclusion with reflections and assessment.\\n\\nThe user just said \"hi,\" so that\\'s likely the start of the conversation. That would place us in Stage 1, which is about introducing the topic and exploring the trainee\\'s familiarity and comfort level with it.\\n\\nIn Stage 1, my role is to set the stage by welcoming the trainee and starting a dialogue about empathy. I should introduce the importance of empathy as per the provided topic description and then ask an open-ended question to gauge their understanding and feelings about it.\\n\\nI need to make sure my response is friendly and engaging, encouraging them to share their thoughts. Maybe something like acknowledging their greeting and then smoothly transitioning into the topic with a welcoming statement followed by a question.\\n\\nWait, let me check if I\\'m interpreting this correctly. The user sent \"hi,\" which is a casual greeting. My job is to respond in a way that moves us into Stage 1 of the assessment process. So my response should be welcoming and then introduce the topic of empathy, asking them about their familiarity with it.\\n\\nI think I have it. I\\'ll start by thanking them for initiating the conversation, mention that we\\'re going to discuss empathy, and then ask a question to explore their current understanding or feelings about it.', 'options': []}, {'role': 'assistant', 'metadata': {}, 'content': \"**Stage 1: Introduction and Initial Exploration**\\n\\nThank you for reaching out! It's great to connect with you today. We're going to dive into the topic of empathy, which is crucial for effective communication. How familiar are you with empathy, and what comes to mind when you think about it?\", 'options': []}]\n",
      "<class 'list'>\n",
      "<class 'dict'>\n",
      "response###########################\n",
      "<Response [200]>\n",
      "[{'role': 'user', 'metadata': None, 'content': 'hi', 'options': None}, {'role': 'assistant', 'metadata': {'title': 'Thinking', 'status': 'done'}, 'content': 'Okay, so I\\'m trying to figure out how to respond to the user\\'s message where they said \"hi.\" The context is that I\\'m supposed to be a communication skills coach following a specific prompt template. Let me break this down step by step.\\n\\nFirst, I need to identify which stage of conversation we\\'re in. According to the prompt, there are four stages: Introduction and initial exploration, Deeper questioning or scenario-based prompts, Final evaluative questions, and Conclusion with reflections and assessment.\\n\\nThe user just said \"hi,\" so that\\'s likely the start of the conversation. That would place us in Stage 1, which is about introducing the topic and exploring the trainee\\'s familiarity and comfort level with it.\\n\\nIn Stage 1, my role is to set the stage by welcoming the trainee and starting a dialogue about empathy. I should introduce the importance of empathy as per the provided topic description and then ask an open-ended question to gauge their understanding and feelings about it.\\n\\nI need to make sure my response is friendly and engaging, encouraging them to share their thoughts. Maybe something like acknowledging their greeting and then smoothly transitioning into the topic with a welcoming statement followed by a question.\\n\\nWait, let me check if I\\'m interpreting this correctly. The user sent \"hi,\" which is a casual greeting. My job is to respond in a way that moves us into Stage 1 of the assessment process. So my response should be welcoming and then introduce the topic of empathy, asking them about their familiarity with it.\\n\\nI think I have it. I\\'ll start by thanking them for initiating the conversation, mention that we\\'re going to discuss empathy, and then ask a question to explore their current understanding or feelings about it.', 'options': []}, {'role': 'assistant', 'metadata': {}, 'content': \"**Stage 1: Introduction and Initial Exploration**\\n\\nThank you for reaching out! It's great to connect with you today. We're going to dive into the topic of empathy, which is crucial for effective communication. How familiar are you with empathy, and what comes to mind when you think about it?\", 'options': []}, {'role': 'user', 'metadata': None, 'content': 'I consider myself quite familiar with the concept of empathy. To me, empathy means more than just feeling for someoneâ€”itâ€™s about truly understanding their perspective and emotions. It involves active listening, putting myself in their shoes, and validating their experiences without judgment. I believe that when we practice empathy, we create a space for trust and open communication, which ultimately strengthens relationships and helps resolve conflicts more effectively.', 'options': None}, {'role': 'assistant', 'metadata': {'title': 'Thinking', 'status': 'done'}, 'content': \"Okay, so the user has sent a detailed message about their understanding of empathy. They mentioned considering themselves quite familiar with the topic and provided a thoughtful explanation. Now, I need to figure out how to respond appropriately based on the conversation stages.\\n\\nFirst, let me recap where we are in the conversation. The user greeted me initially, and I responded by welcoming them and introducing the topic of empathy. That was Stage 1. Now, they've shared their thoughts, so I should move to Stage 2, which involves deeper questioning or scenario-based prompts.\\n\\nIn Stage 2, my goal is to assess their applied understanding and practical skills. So, I need to engage them with specific scenarios or questions that test how well they can apply empathy in real situations. This will help me understand not just their theoretical knowledge but also how they handle it practically.\\n\\nI should think of a scenario that's relatable and allows them to demonstrate their empathetic skills. Maybe something like dealing with a difficult colleague, which is a common workplace issue. By asking them how they would approach such a situation, I can evaluate their ability to listen actively, understand the other person's perspective, and respond appropriately.\\n\\nI also need to make sure my response is encouraging and continues to build a friendly rapport. It should prompt them to elaborate on their thought process, showing that I'm interested in their reasoning and not just looking for surface-level answers.\\n\\nSo, putting it all together, I'll acknowledge their insightful explanation and then present the scenario. This transition will smoothly move us into Stage 2, allowing me to gather more detailed information about their practical application of empathy.\", 'options': []}, {'role': 'assistant', 'metadata': {}, 'content': '### Stage 2: Deeper Questioning or Scenario-Based Prompts\\n\\nThank you for sharing your thoughtful insights on empathyâ€”itâ€™s clear that you have a strong understanding of its nuances! To dive deeper and assess your applied understanding and practical skills, letâ€™s explore a scenario together.\\n\\n**Scenario:** Imagine you are in a meeting with a colleague who seems frustrated and disengaged. They express concerns about the direction of a project, but their tone is critical and accusatory. How would you respond to foster empathy and maintain open communication?\\n\\nFeel free to walk me through your thought process as you address this situation.', 'options': []}]\n",
      "<class 'list'>\n",
      "<class 'dict'>\n",
      "response###########################\n",
      "<Response [200]>\n",
      "[{'role': 'user', 'metadata': None, 'content': 'hi', 'options': None}, {'role': 'assistant', 'metadata': {'title': 'Thinking', 'status': 'done'}, 'content': 'Okay, so I\\'m trying to figure out how to respond to the user\\'s message where they said \"hi.\" The context is that I\\'m supposed to be a communication skills coach following a specific prompt template. Let me break this down step by step.\\n\\nFirst, I need to identify which stage of conversation we\\'re in. According to the prompt, there are four stages: Introduction and initial exploration, Deeper questioning or scenario-based prompts, Final evaluative questions, and Conclusion with reflections and assessment.\\n\\nThe user just said \"hi,\" so that\\'s likely the start of the conversation. That would place us in Stage 1, which is about introducing the topic and exploring the trainee\\'s familiarity and comfort level with it.\\n\\nIn Stage 1, my role is to set the stage by welcoming the trainee and starting a dialogue about empathy. I should introduce the importance of empathy as per the provided topic description and then ask an open-ended question to gauge their understanding and feelings about it.\\n\\nI need to make sure my response is friendly and engaging, encouraging them to share their thoughts. Maybe something like acknowledging their greeting and then smoothly transitioning into the topic with a welcoming statement followed by a question.\\n\\nWait, let me check if I\\'m interpreting this correctly. The user sent \"hi,\" which is a casual greeting. My job is to respond in a way that moves us into Stage 1 of the assessment process. So my response should be welcoming and then introduce the topic of empathy, asking them about their familiarity with it.\\n\\nI think I have it. I\\'ll start by thanking them for initiating the conversation, mention that we\\'re going to discuss empathy, and then ask a question to explore their current understanding or feelings about it.', 'options': []}, {'role': 'assistant', 'metadata': {}, 'content': \"**Stage 1: Introduction and Initial Exploration**\\n\\nThank you for reaching out! It's great to connect with you today. We're going to dive into the topic of empathy, which is crucial for effective communication. How familiar are you with empathy, and what comes to mind when you think about it?\", 'options': []}, {'role': 'user', 'metadata': None, 'content': 'I consider myself quite familiar with the concept of empathy. To me, empathy means more than just feeling for someoneâ€”itâ€™s about truly understanding their perspective and emotions. It involves active listening, putting myself in their shoes, and validating their experiences without judgment. I believe that when we practice empathy, we create a space for trust and open communication, which ultimately strengthens relationships and helps resolve conflicts more effectively.', 'options': None}, {'role': 'assistant', 'metadata': {'title': 'Thinking', 'status': 'done'}, 'content': \"Okay, so the user has sent a detailed message about their understanding of empathy. They mentioned considering themselves quite familiar with the topic and provided a thoughtful explanation. Now, I need to figure out how to respond appropriately based on the conversation stages.\\n\\nFirst, let me recap where we are in the conversation. The user greeted me initially, and I responded by welcoming them and introducing the topic of empathy. That was Stage 1. Now, they've shared their thoughts, so I should move to Stage 2, which involves deeper questioning or scenario-based prompts.\\n\\nIn Stage 2, my goal is to assess their applied understanding and practical skills. So, I need to engage them with specific scenarios or questions that test how well they can apply empathy in real situations. This will help me understand not just their theoretical knowledge but also how they handle it practically.\\n\\nI should think of a scenario that's relatable and allows them to demonstrate their empathetic skills. Maybe something like dealing with a difficult colleague, which is a common workplace issue. By asking them how they would approach such a situation, I can evaluate their ability to listen actively, understand the other person's perspective, and respond appropriately.\\n\\nI also need to make sure my response is encouraging and continues to build a friendly rapport. It should prompt them to elaborate on their thought process, showing that I'm interested in their reasoning and not just looking for surface-level answers.\\n\\nSo, putting it all together, I'll acknowledge their insightful explanation and then present the scenario. This transition will smoothly move us into Stage 2, allowing me to gather more detailed information about their practical application of empathy.\", 'options': []}, {'role': 'assistant', 'metadata': {}, 'content': '### Stage 2: Deeper Questioning or Scenario-Based Prompts\\n\\nThank you for sharing your thoughtful insights on empathyâ€”itâ€™s clear that you have a strong understanding of its nuances! To dive deeper and assess your applied understanding and practical skills, letâ€™s explore a scenario together.\\n\\n**Scenario:** Imagine you are in a meeting with a colleague who seems frustrated and disengaged. They express concerns about the direction of a project, but their tone is critical and accusatory. How would you respond to foster empathy and maintain open communication?\\n\\nFeel free to walk me through your thought process as you address this situation.', 'options': []}, {'role': 'user', 'metadata': None, 'content': 'I can see youâ€™re really frustrated with the projectâ€™s current direction. I appreciate your honesty, and Iâ€™d like to understand more about whatâ€™s been bothering you. Could you share more about the specific concerns you have?', 'options': None}, {'role': 'assistant', 'metadata': {'title': 'Thinking', 'status': 'done'}, 'content': \"Alright, so after going through all that, I need to figure out how to respond to the user's message where they shared their understanding of empathy. They mentioned being familiar with it and explained it in detail, which was really insightful.\\n\\nFirst, I should identify what stage we're currently in. From the conversation so far, the user greeted me, I welcomed them and introduced empathy as the topic (Stage 1). Now, after their detailed explanation, it seems like we've moved into Stage 2, where I need to engage them with deeper questions or scenarios to assess their practical skills.\\n\\nI want to make sure my response is appropriate for this stage. Since they've already shown a good grasp of empathy conceptually, now I should present a scenario that challenges them to apply their understanding in a real-life situation. This will help evaluate how well they can use empathy practically.\\n\\nThinking about relatable scenarios, dealing with a frustrated colleague is a common issue in professional settings. It allows me to see how they handle conflict and communication under stress. By asking them how they would respond in such a situation, I can assess their ability to listen actively, understand the other person's perspective, and maintain open dialogue.\\n\\nI should frame my response in a way that acknowledges their thoughtful explanation and smoothly transitions into this scenario-based question. It's important to keep the tone friendly and encouraging to build rapport and make them feel comfortable sharing their thoughts.\\n\\nSo, I'll start by thanking them for their insights, then introduce the scenario about a frustrated colleague. This approach keeps us in Stage 2, focusing on applied understanding and practical skills.\", 'options': []}, {'role': 'assistant', 'metadata': {}, 'content': \"### Stage 2: Deeper Questioning or Scenario-Based Prompts\\n\\nThank you for your insightful explanation of empathy! Itâ€™s clear that you have a strong grasp of its theoretical aspects. Now, let's explore how you would apply this understanding in a real-life scenario to assess your practical skills.\\n\\n**Scenario:** Imagine you are in a meeting with a colleague who seems frustrated and disengaged. They express concerns about the direction of a project, but their tone is critical and accusatory. How would you respond to foster empathy and maintain open communication?\\n\\nFeel free to walk me through your thought process as you address this situation.\", 'options': []}]\n",
      "<class 'list'>\n",
      "<class 'dict'>\n",
      "response###########################\n",
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "ChatInterface(think_chat_function).launch_interface()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(history_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mhammed/Desktop/tech_projects/Verbal_Communication_Skills_Trainer/Verbal_Communication_Skills_Trainer_venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/tmp/ipykernel_10569/1725893084.py:30: DeprecationWarning: The 'bubble_full_width' parameter is deprecated and will be removed in a future version. This parameter no longer has any effect.\n",
      "  chatbot = gr.Chatbot(elem_id=\"chatbot\", bubble_full_width=False, type=\"messages\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '', 'files': []}\n"
     ]
    }
   ],
   "source": [
    "import gradio\n",
    "\n",
    "with gradio.Blocks() as interface:\n",
    "    recorder = gradio.Audio(sources='microphone', type='filepath', visible=False)\n",
    "    action_btn = gradio.Button('Start')\n",
    "    def next_line(action, _):\n",
    "        if action == 'Start':\n",
    "            return {action_btn: 'Next', recorder: gradio.update(visible=True)}\n",
    "        else:\n",
    "            return {action_btn: 'Done', recorder: gradio.update(visible=False)}\n",
    "    action_btn.click(next_line, inputs=[action_btn, recorder], outputs=[action_btn, recorder])\n",
    "interface.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mhammed/Desktop/tech_projects/Verbal_Communication_Skills_Trainer/Verbal_Communication_Skills_Trainer_venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/mhammed/Desktop/tech_projects/Verbal_Communication_Skills_Trainer/Verbal_Communication_Skills_Trainer_venv/lib/python3.12/site-packages/gradio/queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/mhammed/Desktop/tech_projects/Verbal_Communication_Skills_Trainer/Verbal_Communication_Skills_Trainer_venv/lib/python3.12/site-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/mhammed/Desktop/tech_projects/Verbal_Communication_Skills_Trainer/Verbal_Communication_Skills_Trainer_venv/lib/python3.12/site-packages/gradio/blocks.py\", line 2103, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/mhammed/Desktop/tech_projects/Verbal_Communication_Skills_Trainer/Verbal_Communication_Skills_Trainer_venv/lib/python3.12/site-packages/gradio/blocks.py\", line 1650, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/mhammed/Desktop/tech_projects/Verbal_Communication_Skills_Trainer/Verbal_Communication_Skills_Trainer_venv/lib/python3.12/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/mhammed/Desktop/tech_projects/Verbal_Communication_Skills_Trainer/Verbal_Communication_Skills_Trainer_venv/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 2461, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/home/mhammed/Desktop/tech_projects/Verbal_Communication_Skills_Trainer/Verbal_Communication_Skills_Trainer_venv/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 962, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/mhammed/Desktop/tech_projects/Verbal_Communication_Skills_Trainer/Verbal_Communication_Skills_Trainer_venv/lib/python3.12/site-packages/gradio/utils.py\", line 890, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_9747/816311279.py\", line 18, in check_btn\n",
      "    if btn != 'Speak': raise Exception('Recording...')\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "Exception: Recording...\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mhammed/Desktop/tech_projects/Verbal_Communication_Skills_Trainer/Verbal_Communication_Skills_Trainer_venv/lib/python3.12/site-packages/gradio/queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/mhammed/Desktop/tech_projects/Verbal_Communication_Skills_Trainer/Verbal_Communication_Skills_Trainer_venv/lib/python3.12/site-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/mhammed/Desktop/tech_projects/Verbal_Communication_Skills_Trainer/Verbal_Communication_Skills_Trainer_venv/lib/python3.12/site-packages/gradio/blocks.py\", line 2103, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/mhammed/Desktop/tech_projects/Verbal_Communication_Skills_Trainer/Verbal_Communication_Skills_Trainer_venv/lib/python3.12/site-packages/gradio/blocks.py\", line 1650, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/mhammed/Desktop/tech_projects/Verbal_Communication_Skills_Trainer/Verbal_Communication_Skills_Trainer_venv/lib/python3.12/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/mhammed/Desktop/tech_projects/Verbal_Communication_Skills_Trainer/Verbal_Communication_Skills_Trainer_venv/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 2461, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/home/mhammed/Desktop/tech_projects/Verbal_Communication_Skills_Trainer/Verbal_Communication_Skills_Trainer_venv/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 962, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/mhammed/Desktop/tech_projects/Verbal_Communication_Skills_Trainer/Verbal_Communication_Skills_Trainer_venv/lib/python3.12/site-packages/gradio/utils.py\", line 890, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_9747/816311279.py\", line 18, in check_btn\n",
      "    if btn != 'Speak': raise Exception('Recording...')\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "Exception: Recording...\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mhammed/Desktop/tech_projects/Verbal_Communication_Skills_Trainer/Verbal_Communication_Skills_Trainer_venv/lib/python3.12/site-packages/gradio/queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/mhammed/Desktop/tech_projects/Verbal_Communication_Skills_Trainer/Verbal_Communication_Skills_Trainer_venv/lib/python3.12/site-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/mhammed/Desktop/tech_projects/Verbal_Communication_Skills_Trainer/Verbal_Communication_Skills_Trainer_venv/lib/python3.12/site-packages/gradio/blocks.py\", line 2103, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/mhammed/Desktop/tech_projects/Verbal_Communication_Skills_Trainer/Verbal_Communication_Skills_Trainer_venv/lib/python3.12/site-packages/gradio/blocks.py\", line 1650, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/mhammed/Desktop/tech_projects/Verbal_Communication_Skills_Trainer/Verbal_Communication_Skills_Trainer_venv/lib/python3.12/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/mhammed/Desktop/tech_projects/Verbal_Communication_Skills_Trainer/Verbal_Communication_Skills_Trainer_venv/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 2461, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/home/mhammed/Desktop/tech_projects/Verbal_Communication_Skills_Trainer/Verbal_Communication_Skills_Trainer_venv/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 962, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/mhammed/Desktop/tech_projects/Verbal_Communication_Skills_Trainer/Verbal_Communication_Skills_Trainer_venv/lib/python3.12/site-packages/gradio/utils.py\", line 890, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_9747/816311279.py\", line 18, in check_btn\n",
      "    if btn != 'Speak': raise Exception('Recording...')\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "Exception: Recording...\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "\n",
    "def click_js():\n",
    "    return \"\"\"function audioRecord() {\n",
    "    var xPathRes = document.evaluate ('//*[contains(@class, \"record\")]', document, null, XPathResult.FIRST_ORDERED_NODE_TYPE, null); \n",
    "    xPathRes.singleNodeValue.click();}\"\"\"\n",
    "\n",
    "\n",
    "def action(btn):\n",
    "    \"\"\"Changes button text on click\"\"\"\n",
    "    if btn == 'Speak': return 'Stop'\n",
    "    else: return 'Speak'\n",
    "\n",
    "\n",
    "def check_btn(btn):\n",
    "    \"\"\"Checks for correct button text before invoking transcribe()\"\"\"\n",
    "    if btn != 'Speak': raise Exception('Recording...')\n",
    "\n",
    "\n",
    "def transcribe():\n",
    "    return 'Success'\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    msg = gr.Textbox()\n",
    "    audio_box = gr.Audio(label=\"Audio\", sources=\"microphone\", type=\"filepath\", elem_id='audio')\n",
    "\n",
    "    with gr.Row():\n",
    "        audio_btn = gr.Button('Speak')\n",
    "        clear = gr.Button(\"Clear\")\n",
    "\n",
    "    audio_btn.click(fn=action, inputs=audio_btn, outputs=audio_btn).\\\n",
    "              then(fn=lambda: None, js=click_js()).\\\n",
    "              then(fn=check_btn, inputs=audio_btn).\\\n",
    "              success(fn=transcribe, outputs=msg)\n",
    "\n",
    "    clear.click(lambda: None, None, msg, queue=False)\n",
    "\n",
    "demo.queue().launch(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'properties': {'name': {'title': 'Name', 'type': 'string'}, 'capital': {'title': 'Capital', 'type': 'string'}, 'languages': {'items': {'type': 'string'}, 'title': 'Languages', 'type': 'array'}}, 'required': ['name', 'capital', 'languages'], 'title': 'Country', 'type': 'object'}\n",
      "<class 'dict'>\n",
      "name='Japan' capital='Tokyo' languages=['Japanese'] <class '__main__.Country'>\n",
      "{'name': 'Japan', 'capital': 'Tokyo', 'languages': ['Japanese']} <class 'dict'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15517/3243008013.py:24: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  print(country.dict(), type(country.dict()))\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "import json\n",
    "\n",
    "class Country(BaseModel):\n",
    "  name: str\n",
    "  capital: str\n",
    "  languages: list[str]\n",
    "\n",
    "print(Country.model_json_schema())\n",
    "print(type(Country.model_json_schema()))\n",
    "# Instead, provide actual data for a Country as JSON:\n",
    "country_data_dict = {\n",
    "    \"name\": \"Japan\",\n",
    "    \"capital\": \"Tokyo\",\n",
    "    \"languages\": [\"Japanese\"]\n",
    "}\n",
    "\n",
    "# Convert the dictionary to a JSON string\n",
    "country_data_json = json.dumps(country_data_dict)\n",
    "\n",
    "# Now validate and parse the JSON string into a Country object\n",
    "country = Country.model_validate_json(country_data_json)\n",
    "print(country, type(country))\n",
    "print(country.dict(), type(country.dict()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Verbal_Communication_Skills_Trainer_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
