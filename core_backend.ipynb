{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Exception in threading.excepthook:\n",
      "Exception ignored in thread started by: <bound method Thread._bootstrap of <Thread(Thread-40 (process_prompt), stopped 138638767670976)>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.12/threading.py\", line 1032, in _bootstrap\n",
      "    self._bootstrap_inner()\n",
      "  File \"/usr/lib/python3.12/threading.py\", line 1077, in _bootstrap_inner\n",
      "    self._invoke_excepthook(self)\n",
      "  File \"/usr/lib/python3.12/threading.py\", line 1391, in invoke_excepthook\n",
      "    local_print(\"Exception in threading.excepthook:\",\n",
      "  File \"/home/mhammed/Desktop/tech_projects/Verbal_Communication_Skills_Trainer/Verbal_Communication_Skills_Trainer_venv/lib/python3.12/site-packages/ipykernel/iostream.py\", line 604, in flush\n",
      "    self.pub_thread.schedule(self._flush)\n",
      "  File \"/home/mhammed/Desktop/tech_projects/Verbal_Communication_Skills_Trainer/Verbal_Communication_Skills_Trainer_venv/lib/python3.12/site-packages/ipykernel/iostream.py\", line 267, in schedule\n",
      "    self._event_pipe.send(b\"\")\n",
      "  File \"/home/mhammed/Desktop/tech_projects/Verbal_Communication_Skills_Trainer/Verbal_Communication_Skills_Trainer_venv/lib/python3.12/site-packages/zmq/sugar/socket.py\", line 710, in send\n",
      "    return super().send(data, flags=flags, copy=copy, track=track)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"_zmq.py\", line 1092, in zmq.backend.cython._zmq.Socket.send\n",
      "  File \"_zmq.py\", line 1134, in zmq.backend.cython._zmq.Socket.send\n",
      "  File \"_zmq.py\", line 1209, in zmq.backend.cython._zmq._check_closed\n",
      "zmq.error.ZMQError: Socket operation on non-socket\n",
      "Exception ignored in sys.unraisablehook: <built-in function unraisablehook>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mhammed/Desktop/tech_projects/Verbal_Communication_Skills_Trainer/Verbal_Communication_Skills_Trainer_venv/lib/python3.12/site-packages/ipykernel/iostream.py\", line 604, in flush\n",
      "    self.pub_thread.schedule(self._flush)\n",
      "  File \"/home/mhammed/Desktop/tech_projects/Verbal_Communication_Skills_Trainer/Verbal_Communication_Skills_Trainer_venv/lib/python3.12/site-packages/ipykernel/iostream.py\", line 267, in schedule\n",
      "    self._event_pipe.send(b\"\")\n",
      "  File \"/home/mhammed/Desktop/tech_projects/Verbal_Communication_Skills_Trainer/Verbal_Communication_Skills_Trainer_venv/lib/python3.12/site-packages/zmq/sugar/socket.py\", line 710, in send\n",
      "    return super().send(data, flags=flags, copy=copy, track=track)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"_zmq.py\", line 1092, in zmq.backend.cython._zmq.Socket.send\n",
      "  File \"_zmq.py\", line 1134, in zmq.backend.cython._zmq.Socket.send\n",
      "  File \"_zmq.py\", line 1209, in zmq.backend.cython._zmq._check_closed\n",
      "zmq.error.ZMQError: Socket operation on non-socket\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "import threading\n",
    "\n",
    "# Initialize the Ollama client\n",
    "client = ollama.Client()\n",
    "\n",
    "# Define your batch of prompts\n",
    "prompts = [\n",
    "    \"What is the capital of France?\",\n",
    "    \"Explain the theory of relativity.\",\n",
    "    \"How does photosynthesis work?\"\n",
    "]\n",
    "\n",
    "# Function to process each prompt concurrently\n",
    "def process_prompt(prompt):\n",
    "    # Create a message in the format expected by Ollama\n",
    "    message = {'role': 'user', 'content': prompt}\n",
    "    # Send the message to the model with streaming enabled\n",
    "    stream = client.chat(model='deepseek-r1:14b-qwen-distill-q8_0', messages=[message], stream=True)\n",
    "    \n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    # Process each token as it's generated\n",
    "    for chunk in stream:\n",
    "        token = chunk['message']['content']\n",
    "        if prompt == \"Explain the theory of relativity.\":\n",
    "            print(token, end='', flush=True)\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")  # Separator between responses\n",
    "\n",
    "# Create and start a thread for each prompt\n",
    "threads = []\n",
    "for prompt in prompts:\n",
    "    t = threading.Thread(target=process_prompt, args=(prompt,))\n",
    "    t.start()\n",
    "    threads.append(t)\n",
    "\n",
    "# Wait for all threads to finish\n",
    "for t in threads:\n",
    "    t.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: What is the capital of France?\n",
      "\n",
      "Prompt: Explain the theory of relativity.\n",
      "\n",
      "Prompt: How does photosynthesis work?\n",
      "\n",
      "Prompt: Tell me a brief story about courage\n",
      "\n",
      "Prompt: Tell me a brief story about wisdom\n",
      "\n",
      "Prompt: Tell me a brief story about humor\n",
      "\n",
      "Prompt: Tell me a brief story about humans\n",
      "\n",
      "Prompt: Tell me a brief story about the moon\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
      "pulling 6e9f90f02bb3... 100% ▕████████████████▏ 9.0 GB                         \u001b[K\n",
      "pulling 369ca498f347... 100% ▕████████████████▏  387 B                         \u001b[K\n",
      "pulling 6e4c38e1172f... 100% ▕████████████████▏ 1.1 KB                         \u001b[K\n",
      "pulling f4d24e9138dd... 100% ▕████████████████▏  148 B                         \u001b[K\n",
      "pulling 3c24b0c80794... 100% ▕████████████████▏  488 B                         \u001b[K\n",
      "verifying sha256 digest \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "<think>\n",
      "Alright, so I need to create a brief story about the Moon. Hmm, where do I even start? Well, let's think about what I know about the Moon. It's Earth's natural satellite\n",
      "==================================================\n",
      "\n",
      ", right? It orbits our planet and has been a source of wonder for ages. People have always looked up at it and wondered about its origins and secrets.\n",
      "\n",
      "Maybe I can personify the Moon to make the story more engaging. Personifying objects or celestial bodies often makes stories more relatable. Let me think about what emotions or characteristics the Moon might have. It's often associated with peace, mystery, and sometimes even loneliness because it's up there all alone in the dark sky.\n",
      "\n",
      "So, perhaps the story could follow the Moon as a character who feels isolated but finds purpose by illuminating Earth at night. That gives it a sense of mission and connection to humanity. The idea that without the Moon, nights would be dark and people couldn't see as clearly might tie into themes of guidance or companionship.\n",
      "\n",
      "Wait, I also remember something about phases of the Moon—waxing and waning. Maybe the story can touch on how the Moon changes over time, just like the tides it affects. That adds a dynamic element to its character. But I don't want to get too technical; it should remain a brief, simple narrative.\n",
      "\n",
      "I should also include some elements of nature that interact with the Moon, like the ocean or forests, to show its impact on Earth. Maybe the Moon talks to stars or other celestial beings? That could add depth and interaction in the story.\n",
      "\n",
      "Oh, and how about a turning point where something happens—like a meteor passing by or an eclipse—that shows the Moon's strength or vulnerability. It would make the story more compelling if there's some conflict or challenge that the Moon overcomes.\n",
      "\n",
      "I need to keep the language simple and concise since it's supposed to be brief. Maybe start with introducing the Moon, describe its feelings, show its daily routine of illuminating Earth, then introduce a minor conflict, and resolve it by highlighting the importance of the Moon's role.\n",
      "\n",
      "Wait, but I should make sure not to make it too long. Let me\n",
      "==================================================\n",
      "\n",
      " outline quickly:\n",
      "\n",
      "1. Introduce the Moon as a character in the sky.\n",
      "2. Describe how it feels about being up there alone.\n",
      "3. Show its purpose by illuminating Earth and connecting with people.\n",
      "4. Include interactions with nature elements like stars, ocean, forests.\n",
      "5. Introduce a minor challenge or event that tests the Moon's resolve.\n",
      "6.\n",
      "==================================================\n",
      "\n",
      " Conclude with reaffirming the Moon's role and contentment.\n",
      "\n",
      "I think that structure works. Now, how to put it all together in a flowing manner without being too wordy. Maybe each paragraph can focus on one of these points, keeping sentences short and descriptive.\n",
      "\n",
      "Let me also think about the tone. It should be gentle and reflective, matching the calmness often\n",
      "==================================================\n",
      "\n",
      " associated with the Moon. Avoiding any overly dramatic or intense emotions to keep it aligned with the Moon's serene nature.\n",
      "\n",
      "Hmm, maybe adding a touch of wonder from humans looking up at the Moon could add a nice touch. Show how the Moon feels when people appreciate its light, giving it a sense of accomplishment and purpose.\n",
      "\n",
      "I think I'm ready to draft the story now, keeping all these points in mind.\n",
      "</think>\n",
      "\n",
      "In the vast expanse of space, the Moon floated serenely, casting its gentle glow upon Earth each night. It often felt a deep solitude, being so far removed from the lively world below. Yet, amidst this loneliness, it found purpose in illuminating the dark hours, guiding wanderers and comforting those awake under its light.\n",
      "\n",
      "The Moon shared a silent bond with the stars, who twinkled like old friends nearby. The ocean whispered secrets to it, while forests danced softly in the shadows beneath. Each night, as it waxed and waned, it felt a connection to the Earth's tides, a rhythmic harmony that soothed its heart.\n",
      "\n",
      "One evening, during a total eclipse, darkness encroached upon the Moon's light. Though momentarily obscured, it remained steadfast, knowing that the Sun would soon reveal its face once more. This challenge only reinforced its resolve to continue its gentle\n",
      "==================================================\n",
      "\n",
      " watch over Earth.\n",
      "\n",
      "As people marveled at its beauty, the Moon felt a profound sense of accomplishment. It was not alone in the universe; its presence brought peace and wonder to countless souls below. In this role, it found contentment, knowing that even in solitude, it held a place of significance in the grand celestial dance.\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ollama\n",
    "import subprocess\n",
    "import json\n",
    "import time\n",
    "\n",
    "def load_config(file_path: str):\n",
    "    \"\"\"\n",
    "    Loads environment-variable-like keys from a JSON file and sets them.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        config_data = json.load(f)\n",
    "    for key, value in config_data.items():\n",
    "        # Ensure everything is a string in the environment.\n",
    "        os.environ[key] = str(value)\n",
    "\n",
    "\n",
    "def load_model(model_name: str) -> ollama.Client:\n",
    "    \"\"\"\n",
    "    Instantiates an Ollama client for a given model.\n",
    "    Because environment variables are already set,\n",
    "    Ollama respects those concurrency/queue settings.\n",
    "    \"\"\"\n",
    "    subprocess.run([\"ollama\", \"pull\", model_name], check=True)\n",
    "    return ollama.Client()\n",
    "\n",
    "class ClientSingleton:\n",
    "    _client = None  # Class-level variable to store the singleton instance\n",
    "\n",
    "    @classmethod\n",
    "    def get_client(cls, model_name: str) -> ollama.Client:\n",
    "        \"\"\"\n",
    "        Returns the already instantiated client if it exists,\n",
    "        otherwise creates one using the load_model function.\n",
    "        \"\"\"\n",
    "        if cls._client is None:\n",
    "            cls._client = load_model(model_name)\n",
    "        return cls._client\n",
    "    \n",
    "# 1) Load config file to set environment variables\n",
    "load_config(\"ollama_config.json\")\n",
    "\n",
    "# 2) Create and load your model *once*, after config is applied\n",
    "# client = ClientSingleton.get_client(\"deepseek-r1:14b-qwen-distill-q8_0\")\n",
    "client = ClientSingleton.get_client(\"deepseek-r1:14b-qwen-distill-q4_K_M\")\n",
    "\n",
    "# Function to process each prompt concurrently\n",
    "def process_prompt(prompt):\n",
    "    # Create a message in the format expected by Ollama\n",
    "    message = {'role': 'user', 'content': prompt}\n",
    "    # Send the message to the model with streaming enabled\n",
    "    # stream = client.chat(model='deepseek-r1:14b-qwen-distill-q8_0', messages=[message], stream=True)\n",
    "    stream = client.chat(model='deepseek-r1:14b-qwen-distill-q4_K_M', messages=[message], stream=True)\n",
    "    \n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    # Process each token as it's generated\n",
    "    for chunk in stream:\n",
    "        token = chunk['message']['content']\n",
    "        if prompt == \"Tell me a brief story about the moon\":\n",
    "            print(token, end='', flush=True)\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")  # Separator between responses\n",
    "\n",
    "# Define your batch of prompts\n",
    "prompts = [\n",
    "    \"What is the capital of France?\",\n",
    "    \"Explain the theory of relativity.\",\n",
    "    \"How does photosynthesis work?\",\n",
    "    \"Tell me a brief story about courage\",\n",
    "    \"Tell me a brief story about wisdom\",\n",
    "    \"Tell me a brief story about humor\",\n",
    "    \"Tell me a brief story about humans\",\n",
    "    \"Tell me a brief story about the moon\"\n",
    "]\n",
    "\n",
    "# Create and start a thread for each prompt\n",
    "threads = []\n",
    "for prompt in prompts:\n",
    "    t = threading.Thread(target=process_prompt, args=(prompt,))\n",
    "    t.start()\n",
    "    threads.append(t)\n",
    "\n",
    "# # A shared data structure: one list per prompt to store tokens\n",
    "# outputs = [[] for _ in prompts]\n",
    "# lock = threading.Lock()\n",
    "\n",
    "# def process_prompt(prompt, idx):\n",
    "#     \"\"\"\n",
    "#     Process a prompt: stream tokens from the model and append each token to outputs.\n",
    "#     \"\"\"\n",
    "#     message = {'role': 'user', 'content': prompt}\n",
    "#     # Stream tokens from the model\n",
    "#     stream = client.chat(model='deepseek-r1:14b-qwen-distill-q8_0', messages=[message], stream=True)\n",
    "#     for chunk in stream:\n",
    "#         token = chunk['message']['content']\n",
    "#         with lock:\n",
    "#             outputs[idx].append(token)\n",
    "#         # (Optional) small delay to simulate processing time\n",
    "#         time.sleep(0.05)\n",
    "#     # Mark the prompt as finished\n",
    "#     with lock:\n",
    "#         outputs[idx].append(\"[DONE]\")\n",
    "\n",
    "# def print_grid():\n",
    "#     \"\"\"\n",
    "#     Repeatedly clears the screen and prints a grid where each column represents a prompt and each row a token step.\n",
    "#     \"\"\"\n",
    "#     # Set a fixed column width (adjust as needed)\n",
    "#     col_width = 40\n",
    "#     while True:\n",
    "#         # Clear the screen (works in many ANSI terminals)\n",
    "#         print(\"\\033c\", end=\"\")  # ANSI escape code to clear screen\n",
    "\n",
    "#         # Print a header with the prompt names (truncated if too long)\n",
    "#         header = \"\"\n",
    "#         for prompt in prompts:\n",
    "#             header += prompt[:col_width-2].ljust(col_width)\n",
    "#         print(header)\n",
    "#         print(\"-\" * (col_width * len(prompts)))\n",
    "\n",
    "#         with lock:\n",
    "#             # Determine the maximum number of token steps among all prompts\n",
    "#             max_rows = max(len(col) for col in outputs)\n",
    "#             # Build the grid row by row\n",
    "#             for row in range(max_rows):\n",
    "#                 line_parts = []\n",
    "#                 for col in range(len(outputs)):\n",
    "#                     # If a token exists for this row in this column, use it; else, use an empty string.\n",
    "#                     token = outputs[col][row] if row < len(outputs[col]) else \"\"\n",
    "#                     line_parts.append(token.ljust(col_width))\n",
    "#                 print(\"\".join(line_parts))\n",
    "#             # Check if all prompts have finished by looking for the \"[DONE]\" marker in each output list.\n",
    "#             all_done = all(col and col[-1] == \"[DONE]\" for col in outputs)\n",
    "#         # Pause briefly before reprinting the grid\n",
    "#         time.sleep(0.2)\n",
    "#         if all_done:\n",
    "#             break\n",
    "\n",
    "# # Start a thread for each prompt to process tokens concurrently\n",
    "# threads = []\n",
    "# for i, prompt in enumerate(prompts):\n",
    "#     t = threading.Thread(target=process_prompt, args=(prompt, i))\n",
    "#     t.start()\n",
    "#     threads.append(t)\n",
    "\n",
    "# # Continuously update the grid display in the main thread\n",
    "# print_grid()\n",
    "\n",
    "# # Wait for all threads to finish (they should be finished once \"[DONE]\" is printed for each)\n",
    "# for t in threads:\n",
    "#     t.join()\n",
    "\n",
    "# print(\"\\nAll prompts finished.\")\n",
    "\n",
    "# # Wait for all threads to finish\n",
    "# for t in threads:\n",
    "#     t.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import threading\n",
    "import time\n",
    "\n",
    "# Initialize the Ollama client\n",
    "client = ollama.Client()\n",
    "\n",
    "# Define your batch of prompts\n",
    "prompts = [\n",
    "    \"What is the capital of France?\",\n",
    "    \"Explain the theory of relativity.\",\n",
    "    \"How does photosynthesis work?\"\n",
    "]\n",
    "\n",
    "# A shared data structure: one list per prompt to store tokens\n",
    "outputs = [[] for _ in prompts]\n",
    "lock = threading.Lock()\n",
    "\n",
    "def process_prompt(prompt, idx):\n",
    "    \"\"\"\n",
    "    Process a prompt: stream tokens from the model and append each token to outputs.\n",
    "    \"\"\"\n",
    "    message = {'role': 'user', 'content': prompt}\n",
    "    # Stream tokens from the model\n",
    "    stream = client.chat(model='deepseek-r1:14b-qwen-distill-q8_0', messages=[message], stream=True)\n",
    "    for chunk in stream:\n",
    "        token = chunk['message']['content']\n",
    "        with lock:\n",
    "            outputs[idx].append(token)\n",
    "        # (Optional) small delay to simulate processing time\n",
    "        time.sleep(0.05)\n",
    "    # Mark the prompt as finished\n",
    "    with lock:\n",
    "        outputs[idx].append(\"[DONE]\")\n",
    "\n",
    "def print_grid():\n",
    "    \"\"\"\n",
    "    Repeatedly clears the screen and prints a grid where each column represents a prompt and each row a token step.\n",
    "    \"\"\"\n",
    "    # Set a fixed column width (adjust as needed)\n",
    "    col_width = 40\n",
    "    while True:\n",
    "        # Clear the screen (works in many ANSI terminals)\n",
    "        print(\"\\033c\", end=\"\")  # ANSI escape code to clear screen\n",
    "\n",
    "        # Print a header with the prompt names (truncated if too long)\n",
    "        header = \"\"\n",
    "        for prompt in prompts:\n",
    "            header += prompt[:col_width-2].ljust(col_width)\n",
    "        print(header)\n",
    "        print(\"-\" * (col_width * len(prompts)))\n",
    "\n",
    "        with lock:\n",
    "            # Determine the maximum number of token steps among all prompts\n",
    "            max_rows = max(len(col) for col in outputs)\n",
    "            # Build the grid row by row\n",
    "            for row in range(max_rows):\n",
    "                line_parts = []\n",
    "                for col in range(len(outputs)):\n",
    "                    # If a token exists for this row in this column, use it; else, use an empty string.\n",
    "                    token = outputs[col][row] if row < len(outputs[col]) else \"\"\n",
    "                    line_parts.append(token.ljust(col_width))\n",
    "                print(\"\".join(line_parts))\n",
    "            # Check if all prompts have finished by looking for the \"[DONE]\" marker in each output list.\n",
    "            all_done = all(col and col[-1] == \"[DONE]\" for col in outputs)\n",
    "        # Pause briefly before reprinting the grid\n",
    "        time.sleep(0.2)\n",
    "        if all_done:\n",
    "            break\n",
    "\n",
    "# Start a thread for each prompt to process tokens concurrently\n",
    "threads = []\n",
    "for i, prompt in enumerate(prompts):\n",
    "    t = threading.Thread(target=process_prompt, args=(prompt, i))\n",
    "    t.start()\n",
    "    threads.append(t)\n",
    "\n",
    "# Continuously update the grid display in the main thread\n",
    "print_grid()\n",
    "\n",
    "# Wait for all threads to finish (they should be finished once \"[DONE]\" is printed for each)\n",
    "for t in threads:\n",
    "    t.join()\n",
    "\n",
    "print(\"\\nAll prompts finished.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import lru_cache\n",
    "import numpy as np\n",
    "\n",
    "def quantize_embedding(embedding: np.ndarray, bits: int = 8, group_size: int = 32):\n",
    "    \"\"\"Quantize an embedding using grouped min-max scaling.\"\"\"\n",
    "    assert bits in (4, 8)\n",
    "    max_val = (1 << bits) - 1  # 255 for 8-bit, 15 for 4-bit\n",
    "    embedding = np.array(embedding, dtype=float)\n",
    "    quantized = [len(embedding)]\n",
    "    # Process in groups of group_size\n",
    "    for i in range(0, len(embedding), group_size):\n",
    "        chunk = embedding[i : i + group_size]\n",
    "        m, M = float(chunk.min()), float(chunk.max())\n",
    "        # Scale values of this chunk to [0, max_val]\n",
    "        if M == m:\n",
    "            q_vals = [0] * len(chunk)  # all values equal -> all zeros after quantization\n",
    "        else:\n",
    "            q_vals = [int(round(x)) for x in ((chunk - m) / (M - m) * max_val)]\n",
    "        # Store min, max, and quantized values for this chunk\n",
    "        quantized.append(m); quantized.append(M)\n",
    "        quantized.extend(q_vals)\n",
    "    return tuple(quantized)  # tuple is hashable and can be used as cache key\n",
    "\n",
    "@lru_cache(maxsize=1000)\n",
    "def _cached_llm_inference(qtuple: tuple, bits: int = 8, group_size: int = 32):\n",
    "    \"\"\"Cached LLM inference: returns output for a given quantized embedding.\"\"\"\n",
    "    # **Simulate** an expensive LLM call (here we just return a string result)\n",
    "    return f\"LLM output\"\n",
    "\n",
    "def get_llm_output(embedding: np.ndarray, bits: int = 8, group_size: int = 32):\n",
    "    \"\"\"Return LLM output for embedding, using cache to avoid repeat inference.\"\"\"\n",
    "    qtuple = quantize_embedding(embedding, bits, group_size)\n",
    "    return _cached_llm_inference(qtuple, bits, group_size)\n",
    "\n",
    "# Example usage:\n",
    "emb = np.random.rand(128)\n",
    "print(get_llm_output(emb, bits=8))  # First call (cache miss triggers inference)\n",
    "print(get_llm_output(emb, bits=8))  # Second call (cache hit, returns cached result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cachetools import cached\n",
    "from cachetools.keys import hashkey\n",
    "\n",
    "from random import randint\n",
    "\n",
    "@cached(cache={}, key=lambda db_handle, query: hashkey(query))\n",
    "def find_object(db_handle, query):\n",
    "    print(\"processing {0}\".format(query))\n",
    "    return query\n",
    "\n",
    "queries = list(range(5))\n",
    "queries.extend(range(5))\n",
    "for q in queries:\n",
    "    print(\"result: {0}\".format(find_object(randint(0, 1000), q)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "cache = OrderedDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yo key:  (137, 159, 0, 120, 190, 171, 107, 165, 136, 111, 94, 118, 207, 71, 148, 118, 84, 156, 161, 167, 119, 121, 128, 125, 255, 151, 171, 143, 84, 132, 112, 137, 115, 138, 195, 88, 131, 144, 181, 180, 161, 108, 132, 163, 199, 177, 99, 163, 169, 135, 114, 153, 107, 132, 150, 144, 171, 198, 196, 128, 205, 145, 141, 189, 169, 134, 123, 238, 159, 100, 142, 85, 185, 137, 132, 128, 143, 124, 107, 119, 141, 124, 174, 115, 170, 209, 135, 159, 135, 153, 134, 162, 175, 140, 134, 109, 142, 159, 152, 136, 139, 139, 103, 88, 156, 169, 108, 70, 98, 73, 104, 154, 180, 141, 130, 135, 207, 81, 158, 140, 89, 142, 163, 120, 90, 149, 112, 159, 114, 145, 118, 122, 124, 118, 157, 171, 90, 81, 128, 179, 142, 176, 216, 127, 93, 52, 181, 99, 150, 179, 191, 154, 125, 125, 172, 104, 111, 160, 188, 129, 171, 123, 142, 127, 139, 85, 154, 171, 135, 168, 84, 102, 99, 113, 176, 93, 152, 150, 129, 105, 128, 104, 139, 172, 120, 154, 110, 149, 111, 111, 96, 179, 154, 81, 92, 104, 147, 133, 167, 155, 100, 153, 138, 149, 96, 188, 117, 151, 121, 200, 196, 116, 96, 187, 142, 126, 118, 168, 165, 136, 168, 177, 156, 95, 199, 146, 128, 131, 133, 196, 188, 72, 128, 140, 210, 179, 155, 171, 97, 188, 142, 195, 184, 136, 132, 186, 101, 148, 159, 78, 129, 145, 192, 143, 119, 149, 135, 113, 96, 128, 134, 125, 197, 138, 93, 194, 152, 133, 107, 114, 154, 199, 101, 201, 135, 65, 163, 121, 147, 153, 166, 133, 116, 166, 120, 138, 112, 221, 133, 137, 208, 129, 89, 165, 141, 178, 203, 145, 174, 107, 197, 126, 189, 193, 151, 141, 189, 176, 169, 120, 112, 159, 128, 197, 110, 142, 104, 164, 184, 122, 133, 84, 111, 113, 153, 156, 154, 185, 143, 74, 161, 183, 126, 130, 114, 159, 172, 164, 106, 178, 193, 182, 171, 94, 158, 127, 147, 100, 143, 127, 107, 149, 124, 189, 123, 93, 163, 186, 138, 110, 165, 125, 115, 121, 156, 110, 137, 105, 155, 100, 113, 152, 178, 208, 160, 134, 90, 122, 152, 151, 79, 141, 188, 176, 142, 134, 141, 90, 128, 121, 183, 112, 117, 142, 156, 141, 185, 128, 129, 181, 160, 78, 142, 118, 126, 178, 92, 94, 135, 131, 162, 208, 173, 96, 133, 154, 155, 110, 118, 139, 197, 190, 117, 147, 159, 132, 153, 138, 105, 137, 101, 177, 165, 175, 144, 103, 126, 131, 176, 218, 171, 128, 115, 123, 116, 170, 199, 142, 98, 131, 175, 146, 118, 142, 171, 158, 120, 165, 101, 144, 174, 164, 106, 152, 119, 104, 145, 124, 118, 149, 177, 154, 163, 129, 134, 129, 133, 136, 173, 161, 130, 104, 82, 108, 149, 175, 136, 120, 129, 81, 155, 179, 154, 106, 177, 109, 93, 140, 197, 133, 164, 157, 135, 203, 159, 166, 176, 101, 121, 170, 102, 111, 115, 77, 167, 135, 116, 141, 168, 152, 154, 113, 139, 106, 95, 143, 172, 54, 166, 150, 137, 159, 118, 97, 122, 160, 87, 152, 180, 128, 94, 127, 162, 170, 153, 153, 145, 104, 133, 127, 147, 98, 163, 127, 148, 103, 171, 147, 88, 117, 118, 130, 165, 124, 132, 139, 130, 180, 137, 119, 103, 43, 102, 108, 152, 151, 116, 96, 111, 90, 159, 102, 171, 103, 140, 158, 140, 141, 135, 175, 172, 170, 116, 80, 153, 106, 115, 102, 116, 94, 145, 209, 138, 146, 120, 187, 191, 145, 113, 115, 176, 109, 119, 153, 141, 96, 138, 111, 119, 116, 144, 125, 149, 101, 183, 127, 117, 150, 167, 120, 103, 181, 112, 81, 76, 132, 147, 140, 159, 165, 141, 124, 128, 167, 138, 123, 164, 134, 174, 133, 165, 181, 151, 156, 176, 134, 202, 100, 94, 116, 121, 159, 85, 138, 143, 169, 138, 160, 120, 82, 118, 127, 120, 120, 154, 207, 77, 197, 145, 163, 201, 108, 154, 62, 180, 91, 170, 116, 138, 94, 172, 82, 163, 111, 110, 84, 163, 116, 121, 141, 173, 140, 146, 144, 80, 139, 123, 162, 89, 132, 139, 135, 116, 168, 156, 144, 125, 92, 176, 147, 175, 121, 93, 91, 111, 158, 161, 67, 168, 147, 163, 110, 98, 142, 140, 153, 127, 118, 152, 114, 110, 121, 223, 178, 97, 130, 136, 147, 174, 111, 220, 137, 129, 108, 146, 100, 149, 170, 249, 156, 153, 148, 166, 170, 107, 142, 122, 106)\n",
      "OrderedDict({(137, 159, 0, 120, 190, 171, 107, 165, 136, 111, 94, 118, 207, 71, 148, 118, 84, 156, 161, 167, 119, 121, 128, 125, 255, 151, 171, 143, 84, 132, 112, 137, 115, 138, 195, 88, 131, 144, 181, 180, 161, 108, 132, 163, 199, 177, 99, 163, 169, 135, 114, 153, 107, 132, 150, 144, 171, 198, 196, 128, 205, 145, 141, 189, 169, 134, 123, 238, 159, 100, 142, 85, 185, 137, 132, 128, 143, 124, 107, 119, 141, 124, 174, 115, 170, 209, 135, 159, 135, 153, 134, 162, 175, 140, 134, 109, 142, 159, 152, 136, 139, 139, 103, 88, 156, 169, 108, 70, 98, 73, 104, 154, 180, 141, 130, 135, 207, 81, 158, 140, 89, 142, 163, 120, 90, 149, 112, 159, 114, 145, 118, 122, 124, 118, 157, 171, 90, 81, 128, 179, 142, 176, 216, 127, 93, 52, 181, 99, 150, 179, 191, 154, 125, 125, 172, 104, 111, 160, 188, 129, 171, 123, 142, 127, 139, 85, 154, 171, 135, 168, 84, 102, 99, 113, 176, 93, 152, 150, 129, 105, 128, 104, 139, 172, 120, 154, 110, 149, 111, 111, 96, 179, 154, 81, 92, 104, 147, 133, 167, 155, 100, 153, 138, 149, 96, 188, 117, 151, 121, 200, 196, 116, 96, 187, 142, 126, 118, 168, 165, 136, 168, 177, 156, 95, 199, 146, 128, 131, 133, 196, 188, 72, 128, 140, 210, 179, 155, 171, 97, 188, 142, 195, 184, 136, 132, 186, 101, 148, 159, 78, 129, 145, 192, 143, 119, 149, 135, 113, 96, 128, 134, 125, 197, 138, 93, 194, 152, 133, 107, 114, 154, 199, 101, 201, 135, 65, 163, 121, 147, 153, 166, 133, 116, 166, 120, 138, 112, 221, 133, 137, 208, 129, 89, 165, 141, 178, 203, 145, 174, 107, 197, 126, 189, 193, 151, 141, 189, 176, 169, 120, 112, 159, 128, 197, 110, 142, 104, 164, 184, 122, 133, 84, 111, 113, 153, 156, 154, 185, 143, 74, 161, 183, 126, 130, 114, 159, 172, 164, 106, 178, 193, 182, 171, 94, 158, 127, 147, 100, 143, 127, 107, 149, 124, 189, 123, 93, 163, 186, 138, 110, 165, 125, 115, 121, 156, 110, 137, 105, 155, 100, 113, 152, 178, 208, 160, 134, 90, 122, 152, 151, 79, 141, 188, 176, 142, 134, 141, 90, 128, 121, 183, 112, 117, 142, 156, 141, 185, 128, 129, 181, 160, 78, 142, 118, 126, 178, 92, 94, 135, 131, 162, 208, 173, 96, 133, 154, 155, 110, 118, 139, 197, 190, 117, 147, 159, 132, 153, 138, 105, 137, 101, 177, 165, 175, 144, 103, 126, 131, 176, 218, 171, 128, 115, 123, 116, 170, 199, 142, 98, 131, 175, 146, 118, 142, 171, 158, 120, 165, 101, 144, 174, 164, 106, 152, 119, 104, 145, 124, 118, 149, 177, 154, 163, 129, 134, 129, 133, 136, 173, 161, 130, 104, 82, 108, 149, 175, 136, 120, 129, 81, 155, 179, 154, 106, 177, 109, 93, 140, 197, 133, 164, 157, 135, 203, 159, 166, 176, 101, 121, 170, 102, 111, 115, 77, 167, 135, 116, 141, 168, 152, 154, 113, 139, 106, 95, 143, 172, 54, 166, 150, 137, 159, 118, 97, 122, 160, 87, 152, 180, 128, 94, 127, 162, 170, 153, 153, 145, 104, 133, 127, 147, 98, 163, 127, 148, 103, 171, 147, 88, 117, 118, 130, 165, 124, 132, 139, 130, 180, 137, 119, 103, 43, 102, 108, 152, 151, 116, 96, 111, 90, 159, 102, 171, 103, 140, 158, 140, 141, 135, 175, 172, 170, 116, 80, 153, 106, 115, 102, 116, 94, 145, 209, 138, 146, 120, 187, 191, 145, 113, 115, 176, 109, 119, 153, 141, 96, 138, 111, 119, 116, 144, 125, 149, 101, 183, 127, 117, 150, 167, 120, 103, 181, 112, 81, 76, 132, 147, 140, 159, 165, 141, 124, 128, 167, 138, 123, 164, 134, 174, 133, 165, 181, 151, 156, 176, 134, 202, 100, 94, 116, 121, 159, 85, 138, 143, 169, 138, 160, 120, 82, 118, 127, 120, 120, 154, 207, 77, 197, 145, 163, 201, 108, 154, 62, 180, 91, 170, 116, 138, 94, 172, 82, 163, 111, 110, 84, 163, 116, 121, 141, 173, 140, 146, 144, 80, 139, 123, 162, 89, 132, 139, 135, 116, 168, 156, 144, 125, 92, 176, 147, 175, 121, 93, 91, 111, 158, 161, 67, 168, 147, 163, 110, 98, 142, 140, 153, 127, 118, 152, 114, 110, 121, 223, 178, 97, 130, 136, 147, 174, 111, 220, 137, 129, 108, 146, 100, 149, 170, 249, 156, 153, 148, 166, 170, 107, 142, 122, 106): '<think>\\n\\n</think>\\n\\nThe capital of France is Paris.'})\n",
      "key in it\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "\n",
    "class Singleton(type):\n",
    "    _instances = {}\n",
    "    def __call__(cls, *args, **kwargs):\n",
    "        if cls not in cls._instances:\n",
    "            cls._instances[cls] = super(Singleton, cls).__call__(*args, **kwargs)\n",
    "        return cls._instances[cls]\n",
    "\n",
    "class embedding_wrap(metaclass=Singleton):\n",
    "    def __init__(self, config_file=\"ollama_config.json\"):\n",
    "        self.loaded = False\n",
    "        self.config_data = self.load_config(config_file)\n",
    "        print(self.config_data)\n",
    "        response = self.load_model(self.config_data[\"EMBEDDING_MODEL\"])\n",
    "        if response.status_code == 200: self.loaded = True\n",
    "\n",
    "    def load_config(self, config_file):\n",
    "        \"\"\"\n",
    "        Loads environment-variable-like keys from a JSON file and sets them.\n",
    "        \"\"\"\n",
    "        with open(config_file, 'r') as f:\n",
    "            config_data = json.load(f)\n",
    "        for key, value in config_data.items():\n",
    "            # Ensure everything is a string in the environment.\n",
    "            os.environ[key] = str(value)\n",
    "        return config_data\n",
    "\n",
    "    def load_model(self, model_name: str):\n",
    "        \"\"\"\n",
    "        Instantiates an Ollama client for a given model.\n",
    "        Because environment variables are already set,\n",
    "        Ollama respects those concurrency/queue settings.\n",
    "        \"\"\"\n",
    "        url = self.config_data[\"LOAD_MODEL_API_PATH\"]\n",
    "        payload = {\n",
    "            \"model\": model_name,\n",
    "            \"stream\": False  # Return a single response instead of a streamed response\n",
    "        }\n",
    "\n",
    "        response = requests.post(url, json=payload)\n",
    "        print(\"Status code:\", response.status_code)\n",
    "        print(\"Response:\", response.text)\n",
    "\n",
    "        return response\n",
    "\n",
    "    # def get_client(self, model_name: str) -> ollama.Client:\n",
    "    #     \"\"\"\n",
    "    #     Returns the already instantiated client if it exists,\n",
    "    #     otherwise creates one using the load_model function.\n",
    "    #     Implements a simple singleton for the client.\n",
    "    #     \"\"\"\n",
    "    #     if self.client is None:\n",
    "    #         self.client = self.load_model(model_name)\n",
    "    #     return self.client\n",
    "\n",
    "    def process_input(self, input: str):\n",
    "        \"\"\"\n",
    "        Sends a prompt to the provided Ollama client and prints the \n",
    "        streamed output in real-time.\n",
    "        \"\"\"\n",
    "        # client = self.get_client(model_name)\n",
    "        \n",
    "        # message = {'role': 'user', 'content': prompt}\n",
    "        if self.loaded:\n",
    "            url = self.config_data[\"EMBEDDING_API_PATH\"]\n",
    "            payload = {\n",
    "                        \"model\": self.config_data[\"EMBEDDING_MODEL\"],\n",
    "                        \"input\": input,\n",
    "                        \"keep_alive\": \"10m\",\n",
    "                        \"options\": {\n",
    "                                    \"temperature\": self.config_data[\"EMBED_TEMPERATURE\"]\n",
    "                                    }\n",
    "                        }\n",
    "            response = requests.post(url, json=payload, stream=False)\n",
    "            if response.status_code == 200:\n",
    "                response_data = response.json()\n",
    "            else:\n",
    "                return False\n",
    "            \n",
    "            # print(response_data, type(response_data))\n",
    "\n",
    "            return response_data[\"embeddings\"]\n",
    "            # print(f\"\\nPrompt: {prompt}\")\n",
    "            # for chunk in stream:\n",
    "            #     token = chunk['message']['content']\n",
    "            #     print(token, end='', flush=True)\n",
    "            # print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "        else:\n",
    "            print(\"Failed to load the model\")\n",
    "\n",
    "\n",
    "    def run(self, prompt: str, model_name: str = \"deepseek-r1:14b-qwen-distill-q4_K_M\"):\n",
    "        \"\"\"\n",
    "        Runs the complete workflow: loads config, ensures the model is loaded,\n",
    "        and processes the prompt.\n",
    "        \"\"\"\n",
    "        return self.process_input(prompt)\n",
    "\n",
    "\n",
    "# import numpy as np\n",
    "# embedding = embedding_wrap().run(prompt)\n",
    "# embedding = np.array(embedding) if embedding else embedding\n",
    "# print(embedding)\n",
    "\n",
    "\n",
    "from llm_wrapper import llm_wrap\n",
    "from embedding_wrapper import embedding_wrap\n",
    "import numpy as np\n",
    "import functools\n",
    "\n",
    "class handle_prompt:\n",
    "    def __init__(self, prompt, chat = True):\n",
    "        self.prompt = prompt\n",
    "        self.chat = chat\n",
    "        self.llm = llm_wrap()\n",
    "\n",
    "        self.embedding = embedding_wrap().run(prompt)\n",
    "        self.embedding = np.array(self.embedding) if self.embedding else self.embedding\n",
    "        # print(self.embedding.shape)\n",
    "        # self.embedding = np.array(embedding_wrap().run(prompt))\n",
    "\n",
    "        self.llm_output = self.get_llm_output()\n",
    "\n",
    "    def get_llm_output(self, bits: int = 8, group_size: int = 32):\n",
    "        \"\"\"Return LLM output for embedding, using cache to avoid repeat inference.\"\"\"\n",
    "        if not isinstance(self.embedding, np.ndarray): return self.llm.process_prompt(self.prompt, chat = self.chat)\n",
    "        embedding = self.quantize_embedding(self.embedding)\n",
    "        # return _cached_llm_inference(self.prompt, self.llm, qtuple, bits, group_size)\n",
    "        return _cached_llm_inference(self.prompt, self.llm, self.chat, embedding)\n",
    "    \n",
    "    def quantize_embedding(self, embedding: np.ndarray, n_levels: int = 256, min_val: float = None, max_val: float = None) -> np.ndarray:\n",
    "        # Determine the quantization range.\n",
    "        if min_val is None:\n",
    "            min_val = np.min(embedding)\n",
    "        if max_val is None:\n",
    "            max_val = np.max(embedding)\n",
    "        \n",
    "        # Avoid division by zero for constant embeddings.\n",
    "        if min_val == max_val:\n",
    "            return np.zeros_like(embedding, dtype=np.uint8)\n",
    "        \n",
    "        # Normalize the embedding to the [0, 1] range.\n",
    "        normalized = (embedding - min_val) / (max_val - min_val)\n",
    "        \n",
    "        # Scale and round to the desired number of quantization levels.\n",
    "        quantized_embedding = np.round(normalized * (n_levels - 1)).astype(np.uint8)[0].tolist()#[0]\n",
    "        # print(quantized_embedding)\n",
    "        \n",
    "        return tuple(quantized_embedding)\n",
    "\n",
    "    # # Example usage:\n",
    "    # # emb = np.random.rand(128)\n",
    "    # print(get_llm_output(self.embedding, bits=8))  # First call (cache miss triggers inference)\n",
    "    # print(get_llm_output(self.embedding, bits=8))  # Second call (cache hit, returns cached result)\n",
    "\n",
    "\n",
    "def lru_cache_by_key(key_func, maxsize=128):\n",
    "    \"\"\"\n",
    "    Decorator that mimics lru_cache but uses a custom key function.\n",
    "    The key_func accepts the same arguments as the decorated function\n",
    "    and returns a hashable cache key.\n",
    "    \"\"\"\n",
    "    \n",
    "    def decorating_function(user_function):\n",
    "\n",
    "        @functools.wraps(user_function)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            key = key_func(*args, **kwargs)\n",
    "            print(\"yo key: \",key)\n",
    "            print(cache)\n",
    "            if key in cache:\n",
    "                print(\"key in it\")\n",
    "                cache.move_to_end(key)  # Mark as recently used\n",
    "                return cache[key]\n",
    "\n",
    "            result = user_function(*args, **kwargs)\n",
    "            print(\"yo result: \",result)\n",
    "            cache[key] = result\n",
    "\n",
    "            if len(cache) > maxsize:\n",
    "                cache.popitem(last=False)  # Remove least recently used\n",
    "\n",
    "            return result\n",
    "\n",
    "        wrapper.cache_clear = cache.clear\n",
    "        return wrapper\n",
    "\n",
    "    return decorating_function\n",
    "\n",
    "\n",
    "@lru_cache_by_key(key_func=lambda prompt, llm, chat, embedding: embedding, maxsize=128)\n",
    "def _cached_llm_inference(prompt, llm, chat, embedding):\n",
    "    \"\"\"Cached LLM inference: returns output for a given quantized embedding.\"\"\"\n",
    "    print(\"Computing inference...\")\n",
    "    print(\"Embedding:\", embedding)\n",
    "    return llm.process_prompt(prompt, chat=chat)\n",
    "\n",
    "\n",
    "\n",
    "llm_response = handle_prompt(\"What is the capital of France?\", chat = True)\n",
    "print(llm_response.llm_output)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n"
     ]
    }
   ],
   "source": [
    "cache = OrderedDict()\n",
    "cache[\"kid\"] = \"shot\"\n",
    "if \"kid\" in cache:\n",
    "    print(\"here\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Verbal_Communication_Skills_Trainer_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
